{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc24f3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import keyboard\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3669df3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(tflite_save_path):\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_save_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    return interpreter, input_details, output_details\n",
    "\n",
    "def gesture_preprocess(landmark):\n",
    "    \"\"\"\n",
    "    convert landmarks for trainable data\n",
    "    66 features\n",
    "    X (21): 0-20\n",
    "    Y (21): 21-41\n",
    "    Z (21): 42-62\n",
    "    X,Y,Z range (3): 63-65\n",
    "\n",
    "    params landmark: mediapipe landmark for 1 hand\n",
    "    params label: str\n",
    "    return: np.array (1,66)\n",
    "    \"\"\"\n",
    "    lm_x = np.array([])\n",
    "    lm_y = np.array([])\n",
    "    lm_z = np.array([])\n",
    "    for hlm in landmark.landmark:\n",
    "        lm_x = np.append(lm_x, hlm.x)\n",
    "        lm_y = np.append(lm_y, hlm.y)\n",
    "        lm_z = np.append(lm_z, hlm.z)\n",
    "    data_gest = [lm_x, lm_y, lm_z]\n",
    "    x_rng, y_rng, z_rng = lm_x.max() - lm_x.min(), lm_y.max() - lm_y.min(), lm_z.max() - lm_z.min()\n",
    "    data_gest = np.ravel([(k - k.min()) / (k.max() - k.min()) for i, k in enumerate(data_gest)])\n",
    "    data_gest = np.append(data_gest, [x_rng, y_rng, z_rng])\n",
    "    return data_gest.astype('float32')\n",
    "\n",
    "def gesture_inference(data):\n",
    "    \"\"\"\n",
    "    inference\n",
    "\n",
    "    param data: np.array\n",
    "    return: int class\n",
    "    \"\"\"\n",
    "    interpreter.set_tensor(input_details[0]['index'], np.array([data]))\n",
    "    interpreter.invoke()\n",
    "    tflite_results = interpreter.get_tensor(output_details[0]['index'])\n",
    "    inf_idx = np.argmax(np.squeeze(tflite_results))\n",
    "    if np.squeeze(tflite_results)[inf_idx] < 0.95:\n",
    "        return -1\n",
    "    return inf_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e70c6db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gesture_pipeline(image, gest_time, hands, mp_hands, mp_drawing, mp_drawing_styles, debug=True):\n",
    "    \"\"\"\n",
    "    param image: stream image\n",
    "    param gest_time: timer\n",
    "    param debug: bool - debug view\n",
    "    return int: gesture id\n",
    "    return image: drawn image\n",
    "    return time: updated timer\n",
    "    \"\"\"\n",
    "    inf_class = {-1: 'None', 0: 'Hit', 1: 'Stand', 2: 'Split', 3: 'Reset'}\n",
    "    inf_class_idx = -1\n",
    "    \n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image)\n",
    "    \n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    if results.multi_hand_landmarks:\n",
    "        if (time.time() - gest_time) > 0.5:\n",
    "#                 print(\"detected hand\")\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    # inference\n",
    "                    gest_data = gesture_preprocess(hand_landmarks)\n",
    "                    inf_class_idx = gesture_inference(gest_data)\n",
    "                    \n",
    "                    if debug:\n",
    "                        # draw\n",
    "                        mp_drawing.draw_landmarks(\n",
    "                            image,\n",
    "                            hand_landmarks,\n",
    "                            mp_hands.HAND_CONNECTIONS,\n",
    "                            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                            mp_drawing_styles.get_default_hand_connections_style())\n",
    "    else:\n",
    "        gest_time = time.time()\n",
    "    return inf_class[inf_class_idx], image, gest_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2140f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtsp_url = \"rtsp://192.168.1.98:8554/unicast\"\n",
    "stream_video = cv2.VideoCapture(rtsp_url)\n",
    "\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2aca961",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "tflite_save_path = 'blackbeard/gesture/model/model.tflite'\n",
    "interpreter, input_details,output_details = load_model(tflite_save_path)\n",
    "\n",
    "# inf_class = {-1: 'None', 0: 'Hit', 1: 'Stand', 2: 'Split', 3: 'Reset'}\n",
    "# inf_class_idx = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "618ec487",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp_hands.Hands(\n",
    "    max_num_hands=1,\n",
    "    model_complexity=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "    detect_time = time.time()\n",
    "    while stream_video.isOpened():\n",
    "\n",
    "        # Reading image from stream\n",
    "        _, image = stream_video.read()\n",
    "\n",
    "        ########################################################\n",
    "        # ######### START OBJECT DETECTION PIPELINE #########  #\n",
    "\n",
    "        # Get detected objects from stream\n",
    "#         for detected_objects in object_detection(net, obj_labels, image, cuda=1):\n",
    "\n",
    "#             print(\"[INFO] Card Detected:\", detected_objects)  # for debug\n",
    "#             print(\"---------------------------------------------\")  # for debug\n",
    "\n",
    "        # ########## END OBJECT DETECTION PIPELINE ##########  #\n",
    "        ########################################################\n",
    "\n",
    "        ########################################################\n",
    "        # ############## START GESTURE PIPELINE #############  #\n",
    "        \n",
    "        gest_class, image, detect_time = gesture_pipeline(image, \n",
    "                                                          detect_time, \n",
    "                                                          hands, \n",
    "                                                          mp_hands, \n",
    "                                                          mp_drawing, \n",
    "                                                          mp_drawing_styles, \n",
    "                                                          debug)\n",
    "#         image.flags.writeable = False\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#         results = hands.process(image)\n",
    "        \n",
    "#         image.flags.writeable = True\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "#         if results.multi_hand_landmarks:\n",
    "#             if (time.time() - detect_time) > 0.5:\n",
    "#                     print(\"detected hand\")\n",
    "#                     for hand_landmarks in results.multi_hand_landmarks:\n",
    "#                         # inference\n",
    "#                         gest_data = gesture_preprocess(hand_landmarks)\n",
    "#                         inf_class_idx = gesture_inference(gest_data)\n",
    "\n",
    "#                         # draw\n",
    "#                         mp_drawing.draw_landmarks(\n",
    "#                             image,\n",
    "#                             hand_landmarks,\n",
    "#                             mp_hands.HAND_CONNECTIONS,\n",
    "#                             mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "#                             mp_drawing_styles.get_default_hand_connections_style())\n",
    "#         else:\n",
    "#             detect_time = time.time()\n",
    "\n",
    "        # ############### END GESTURE PIPELINE ##############  #\n",
    "        ########################################################\n",
    "\n",
    "        ########################################################\n",
    "        # ######## START BLACKJACK STRATEGY PIPELINE ########  #\n",
    "\n",
    "        # Insert code here\n",
    "        # ######### END BLACKJACK STRATEGY PIPELINE #########  #\n",
    "        ########################################################\n",
    "\n",
    "        # debug view\n",
    "        if debug:\n",
    "            cv2.putText(image, f\"{gest_class}\", (0, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "            cv2.imshow('Debug View', image)\n",
    "            \n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "stream_video.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
